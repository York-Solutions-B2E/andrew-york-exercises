{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want base data and target data\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# These var names are convention for base and target\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "print(y)\n",
    "print(data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data.data, columns = data.feature_names)\n",
    "\n",
    "print(df.head())\n",
    "# df.to_csv('rawdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(426, 30)\n",
      "(143, 30)\n"
     ]
    }
   ],
   "source": [
    "# MUST Split training and testing data otherwise you'll overfit\n",
    "\n",
    "# Test size is % of total. All remaining data goes into train set\n",
    "# Shuffle the data with random_state to minimize bias from the dataset's creation. value is a seed\n",
    "# The method shuffles data by default but the seed is needed to get the same shuffle repeatedly\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=16)\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1\n",
      " 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1\n",
      " 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1]\n",
      "[1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0\n",
      " 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1\n",
      " 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1]\n",
      "acc: 0.916083916083916\n",
      "prc: 0.9157894736842105\n",
      "rec: 0.9560439560439561\n",
      "[[44  8]\n",
      " [ 4 87]]\n"
     ]
    }
   ],
   "source": [
    "# K nearest neighbor\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train,y_train)\n",
    "knn_pred = knn_model.predict(X_test)\n",
    "\n",
    "print(knn_pred)\n",
    "print(y_test)\n",
    "\n",
    "acc = accuracy_score(y_test, knn_pred)\n",
    "print('acc:', acc) # % of accurate predictions (true hits over all hits) (TP + TN) / (TP + TN + FP + FN)\n",
    "prc = precision_score(y_test, knn_pred) # Percent of accurate positives (TP / (TP + FP))\n",
    "print('prc:', prc)\n",
    "rec = recall_score(y_test, knn_pred) # How many cases you missed. TP / (TP + FN)\n",
    "print('rec:', rec)\n",
    "# These all need to be as high as possible\n",
    "\n",
    "cfu = confusion_matrix(y_test, knn_pred) # As many hits as possible on the diagonal is the goal\n",
    "# [TP FP]\n",
    "# [FN TN]\n",
    "print(cfu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore what a k neighbors classifier is; what parameters can you give it to increase scores\n",
    "Instance-based / non-generalizing\n",
    "\n",
    "kNN calculates the distance between a sample point and all other training data points. Can be straight line distance or taxicab. Selects k nearest to compare\n",
    "(\"Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\")\n",
    "\n",
    "\n",
    "Higher k = smoother decision boundary, lower variance, higher bias, more time\n",
    "Lower k = less time, more noise impact, more overfitting likelihood\n",
    "Cross Validation: Try many values of k\n",
    "\n",
    "By default, neighbors are uniformly weighted. Can be changed to distance weighting: weight proportional to inverse distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
